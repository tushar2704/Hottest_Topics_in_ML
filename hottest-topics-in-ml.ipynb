{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tusharaggarwal27/hottest-topics-in-ml?scriptVersionId=124033235\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:20px;\n           background-color:#F5DEB3;\n           font-size:180%;\n           font-family:Helvetica, Sans-Serif;\n           letter-spacing:0.5px\">\n<p style=\"padding: 10px;\n          text-align: center;\n          font-size:150%;\n          color:blue;\">\n           üå∂Ô∏è‚ô®Ô∏èüíπHottest Topics in MLüå∂Ô∏è‚ô®Ô∏èüíπ\n            \n</p>\n<style>\n        h1{text-align: center;}\n </style>  \n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:cursive; font-size:25px; color:'darkcyan';\">I brewed this notebook from scratch, If this notebook helped, please consider upvoting and cite me if sharing ,Thank you!</p>\n\n\n<p style=\"width: 700px;padding: 20px;background: papayawhip;border-radius:10px;font-size:23px;font-size:23px;border-radius:20px\">\n    <a href=\"https://www.linkedin.com/in/tusharaggarwalinseec/\" target=\"_blank\">Lets connect on LinkedIn!</a>\n    \n   </p>\n<p style=\"width: 700px;padding: 20px;background: papayawhip;border-radius:10px;font-size:23px;border-radius:20px\">\n<a href=\"https://github.com/tushar2704\" target=\"_blank\">Follow me on Github too!</a> </p>\n\n<p style=\"width: 700px;padding: 20px;background: papayawhip;border-radius:10px;font-size:23px;border-radius:20px\">\n    <a href=\"https://medium.com/@tushar_aggarwal\" target=\"_blank\">Also checkout my Medium posts!</a>\n    \n   </p>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\" style=\"font-size:17px;border-radius:20px\">\n    <b>In this project, I am using Natural Language Processing on NIPS papers to uncover the trendiest topics in machine learning research,</b>\n    \n</div>\n<div class=\"alert alert-info\" role=\"alert\" style=\"font-size:15px;border-radius:20px\">\n    <b>Some things to note:</b>\n    <br><br>Neural Information Processing Systems (NIPS) is one of the top machine learning conferences in the world where groundbreaking work is published. \n    <br><br>\n    In this project, I am going to analyze a large collection of NIPS research papers from the past decade to discover the latest trends in machine learning.\n    <br><br>\n    The techniques used here to handle large amounts of data can be applied to other text datasets as well.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"width: 700px;padding: 20px;background: papayawhip;border-radius:10px;font-size:17px;border-radius:20px\">Importing the required Libraries\n\n \n\n    \n   </p>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport re\nimport wordcloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nimport warnings\nwarnings.simplefilter(\"ignore\", DeprecationWarning)\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA","metadata":{"execution":{"iopub.status.busy":"2023-01-16T08:28:24.253143Z","iopub.execute_input":"2023-01-16T08:28:24.253921Z","iopub.status.idle":"2023-01-16T08:28:25.322857Z","shell.execute_reply.started":"2023-01-16T08:28:24.253823Z","shell.execute_reply":"2023-01-16T08:28:25.321901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\" style=\"font-size:17px;border-radius:20px\">\n    <b>Loading the NIPS papers,</b>\n    \n</div>\n<div class=\"alert alert-info\" role=\"alert\" style=\"font-size:15px;border-radius:20px\">\n    <b>Some things to note:</b>\n    <br><br>The NIPS conference (Neural Information Processing Systems) is one of the most prestigious yearly events in the machine learning community. At each NIPS conference, a large number of research papers are published. Over 50,000 PDF files were automatically downloaded and processed to obtain a dataset on various machine learning techniques. These NIPS papers are stored in datasets/papers.csv. The CSV file contains information on the different NIPS papers that were published from 1987 until 2017 (30 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods and many more.\n    <br><br>\n    A research paper typically consists of a title, an abstract and the main text. Other data such as figures and tables were not extracted from the PDF files. Each paper discusses a novel technique or improvement. In this analysis, we will focus on analyzing these papers with natural language processing methods.\n</div>","metadata":{}},{"cell_type":"code","source":"# Reading papers.csv into papers\npapers = pd.read_csv(\"/kaggle/input/nips-papers/papers.csv\")\n\n# Printing out the first rows of papers\npapers.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-16T08:28:25.325448Z","iopub.execute_input":"2023-01-16T08:28:25.326301Z","iopub.status.idle":"2023-01-16T08:28:30.498115Z","shell.execute_reply.started":"2023-01-16T08:28:25.326252Z","shell.execute_reply":"2023-01-16T08:28:30.49701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"width: 700px;padding: 20px;background: papayawhip;border-radius:10px;font-size:17px;border-radius:20px\">Preparing the data for analysis\n\n \n\n    \n   </p>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\" style=\"font-size:15px;border-radius:20px\">\n   For the analysis of these papers, I will use the text data associated with the paper as well as the year the paper was published in.\n    <br><br>\n    Also I will analyze this text data using natural language processing. Since the file contains some metadata such as id's and filenames, it can be removed .\n</div>","metadata":{}},{"cell_type":"code","source":"# Removing the columns\npapers.drop(['id','event_type','pdf_name'],axis=1,inplace=True)\n\n# Printing for checking the first rows of papers\npapers.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-16T08:28:30.499416Z","iopub.execute_input":"2023-01-16T08:28:30.499766Z","iopub.status.idle":"2023-01-16T08:28:30.520632Z","shell.execute_reply.started":"2023-01-16T08:28:30.499735Z","shell.execute_reply":"2023-01-16T08:28:30.519401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"width: 700px;padding: 20px;background: papayawhip;border-radius:10px;font-size:17px;border-radius:20px\">Plotting how machine learning has evolved over time\n\n   </p>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\" style=\"font-size:15px;border-radius:20px\">\n  Visualizing the number of publications per year.\n    <br><br>\n    From this I understood the extent of the machine learning 'revolution'!this significant increase in popularity is attributed to the large amounts of compute power, data and improvements in algorithms.\n</div>","metadata":{}},{"cell_type":"code","source":"# Groupping the papers by year\ngroups = papers.groupby('year')\n\n# Determining the size of each group\ncounts = groups.size()\n\n# Visualising the counts as a bar plot\ncounts.plot(kind='bar')","metadata":{"execution":{"iopub.status.busy":"2023-01-16T08:28:30.522838Z","iopub.execute_input":"2023-01-16T08:28:30.523292Z","iopub.status.idle":"2023-01-16T08:28:30.956965Z","shell.execute_reply.started":"2023-01-16T08:28:30.523261Z","shell.execute_reply":"2023-01-16T08:28:30.9559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"width: 700px;padding: 20px;background: papayawhip;border-radius:10px;font-size:17px;border-radius:20px\">Preprocessing the text data\n\n   </p>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\" style=\"font-size:15px;border-radius:20px\">\n  Analyzing the titles of the different papers to identify machine learning trends\n    <br><br>\n    I am using regular expression(re) to remove any punctuation in the title, then performing lowercasing\n\n</div>","metadata":{}},{"cell_type":"code","source":"# Printing the titles of the first rows \nprint(papers['title'].head())\n\n# Removing the punctuation\npapers['title_processed'] = papers['title'].map(lambda x: re.sub('[,\\.!?]', '', x))\n\n# Converting now the titles to lowercase\npapers['title_processed'] = papers['title_processed'].map(lambda x: x.lower())\n\n# Printing now the processed titles of the first rows \npapers['title_processed'].head()","metadata":{"execution":{"iopub.status.busy":"2023-01-16T08:28:30.958759Z","iopub.execute_input":"2023-01-16T08:28:30.959153Z","iopub.status.idle":"2023-01-16T08:28:30.986778Z","shell.execute_reply.started":"2023-01-16T08:28:30.95912Z","shell.execute_reply":"2023-01-16T08:28:30.985617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"width: 700px;padding: 20px;background: papayawhip;border-radius:10px;font-size:17px;border-radius:20px\">A word cloud to visualize the preprocessed text data\n\n   </p>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\" style=\"font-size:15px;border-radius:20px\">\n    For most common words, using WordCloud\n   \n</div>","metadata":{}},{"cell_type":"code","source":"# Joining the different processed titles togetheras required by WordCloud\nlong_string = ' '.join(papers['title_processed'])\n\n# Creating a WordCloud object, generate a wordcloud and visualising it\nwordcloud = wordcloud.WordCloud()\n\n# Generating a word cloud\nwordcloud.generate(long_string)\n\n# Visualizing the word cloud to see preprocessing\nwordcloud.to_image()","metadata":{"execution":{"iopub.status.busy":"2023-01-16T08:28:30.988206Z","iopub.execute_input":"2023-01-16T08:28:30.988542Z","iopub.status.idle":"2023-01-16T08:28:31.565868Z","shell.execute_reply.started":"2023-01-16T08:28:30.988511Z","shell.execute_reply":"2023-01-16T08:28:31.565047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"width: 700px;padding: 20px;background: papayawhip;border-radius:10px;font-size:17px;border-radius:20px\">Preparing the text for LDA analysis\n\n   </p>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\" style=\"font-size:15px;border-radius:20px\">\nThe main text analysis method that I will be using the latent Dirichlet allocation (LDA). LDA is able to perform topic detection on large document sets, determining what the main 'topics' are in a large unlabeled set of texts. A 'topic' is a collection of words that tend to co-occur often. The hypothesis is that LDA might be able to clarify what the different topics in the research titles are. These topics can then be used as a starting point for further analysis.\n    <br><br>\n    LDA does not work directly on text data. First, it is necessary to convert the documents into a simple vector representation. This representation will then be used by LDA to determine the topics. Each entry of a 'document vector' will correspond with the number of times a word occurred in the document. In conclusion, we will convert a list of titles into a list of vectors, all with length equal to the vocabulary. So, for Analyzing machine learning trends with neural networks.' would be transformed into [1, 0, 1, ..., 1, 0].\n    <br><br>\n   I will then plot the 10 most common words based on the outcome of this operation (the list of document vectors). As a check, these words should also occur in the word cloud.\n   \n</div>","metadata":{}},{"cell_type":"code","source":"# Defining the Helper function\ndef plot_10_most_common_words(count_data, count_vectorizer):\n    import matplotlib.pyplot as plt\n    words = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts+=t.toarray()[0]\n    \n    count_dict = (zip(words, total_counts))\n    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n    words = [w[0] for w in count_dict]\n    counts = [w[1] for w in count_dict]\n    x_pos = np.arange(len(words)) \n\n    plt.bar(x_pos, counts,align='center')\n    plt.xticks(x_pos, words, rotation=90) \n    plt.xlabel('words')\n    plt.ylabel('counts')\n    plt.title('10 most common words')\n    plt.show()\n\n# Initialising the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')\n\n# Fitting and transformin the processed titles\ncount_data = count_vectorizer.fit_transform(papers['title_processed'])\n\n# Visualising now the 10 most common words\nplot_10_most_common_words(count_data, count_vectorizer)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T08:28:31.567356Z","iopub.execute_input":"2023-01-16T08:28:31.568081Z","iopub.status.idle":"2023-01-16T08:28:32.358344Z","shell.execute_reply.started":"2023-01-16T08:28:31.567963Z","shell.execute_reply":"2023-01-16T08:28:32.357263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"width: 700px;padding: 20px;background: papayawhip;border-radius:10px;font-size:17px;border-radius:20px\">Now, Analysing trends with LDA\n\n   </p>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\" style=\"font-size:15px;border-radius:20px\">\nThe research titles, I will be analyzing using LDA. Note that in order to process a new set of documents (e.g. news articles), a similar set of steps will be required to preprocess the data. The flow that was constructed here can thus easily be exported for a new text dataset.\n    <br><br>\n    The only parameter that needs tweaking is the number of topics in the LDA algorithm. Typically, one would calculate the 'perplexity' metric to determine which number of topics is best and iterate over different amounts of topics until the lowest 'perplexity' is found. For now, trying with a different number of topics. From there, I can distinguish what each topic is about ('neural networks', 'reinforcement learning', 'kernel methods', 'gaussian processes', etc.).\n   </div>","metadata":{}},{"cell_type":"code","source":"#Defining the Helper function\ndef print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n        \n# Tweaking the two parameters below (use int values below 15)\nnumber_topics = 10\nnumber_words = 10\n\n# Creating and fitting the LDA model\nlda = LDA(n_components=number_topics)\nlda.fit(count_data)\n\n# Printing the topics found by the LDA model\nprint(\"Topics found via LDA:\")\nprint_topics(lda, count_vectorizer, number_words)","metadata":{"execution":{"iopub.status.busy":"2023-01-16T08:28:32.359506Z","iopub.execute_input":"2023-01-16T08:28:32.359817Z","iopub.status.idle":"2023-01-16T08:28:43.237218Z","shell.execute_reply.started":"2023-01-16T08:28:32.359789Z","shell.execute_reply":"2023-01-16T08:28:43.235958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"width: 700px;padding: 20px;background: papayawhip;border-radius:10px;font-size:17px;border-radius:20px\">The future of machine learning starts with these papers\n\n   </p>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-info\" role=\"alert\" style=\"font-size:15px;border-radius:20px\">\nMachine learning has become increasingly popular over the past years. The number of NIPS conference papers has risen exponentially, and people are continuously looking for ways on how they can incorporate machine learning into their products and services.\n    <br><br>\n    Although this analysis focused on analyzing machine learning trends in research, a lot of these techniques are rapidly being adopted in industry.\n   </div>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:cursive; font-size:25px; color:'darkcyan';\">I brewed this notebook from scratch, If this notebook helped, please consider upvoting and cite me if sharing ,Thank you!</p>\n‚Äã\n‚Äã\n<p style=\"width: 700px;padding: 20px;background: papayawhip;border-radius:10px;font-size:23px;font-size:23px;border-radius:20px\">\n    <a href=\"https://www.linkedin.com/in/tusharaggarwalinseec/\" target=\"_blank\">Lets connect on LinkedIn!</a>\n    \n   </p>\n<p style=\"width: 700px;padding: 20px;background: papayawhip;border-radius:10px;font-size:23px;border-radius:20px\">\n<a href=\"https://github.com/tushar2704\" target=\"_blank\">Follow me on Github too!</a> </p>\n‚Äã\n<p style=\"width: 700px;padding: 20px;background: papayawhip;border-radius:10px;font-size:23px;border-radius:20px\">\n    <a href=\"https://medium.com/@tushar_aggarwal\" target=\"_blank\">Also checkout my Medium posts!</a>\n    \n   </p>","metadata":{}}]}